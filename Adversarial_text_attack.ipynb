{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install allennlp==1.0.0\n",
    "!pip install allennlp-models==1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from copy import deepcopy\n",
    "from allennlp.data import DataLoader\n",
    "from allennlp.data.samplers import BucketBatchSampler\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.nn.util import get_text_field_mask\n",
    "from allennlp.training.metrics import CategoricalAccuracy, F1Measure\n",
    "from allennlp.training.trainer import GradientDescentTrainer\n",
    "from allennlp_models.classification.dataset_readers.stanford_sentiment_tree_bank import \\\n",
    "    StanfordSentimentTreeBankDatasetReader\n",
    "\n",
    "#from allennlp.common.util import lazy_groups_of\n",
    "from allennlp.modules.token_embedders.embedding import _read_pretrained_embeddings_file\n",
    "from allennlp.data.token_indexers import SingleIdTokenIndexer\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder\n",
    "from allennlp.nn.util import move_to_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmClassifier(Model):\n",
    "    def __init__(self, embedder, encoder, vocab):  \n",
    "        super().__init__(vocab)\n",
    "\n",
    "        self.embedder = embedder\n",
    "\n",
    "        self.encoder = encoder\n",
    "\n",
    "        self.linear = torch.nn.Linear(in_features=encoder.get_output_dim(),\n",
    "                                          out_features=vocab.get_vocab_size('labels'))\n",
    "        \n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, tokens, label=None):\n",
    "        \n",
    "        mask = get_text_field_mask(tokens)\n",
    "\n",
    "        # Forward pass\n",
    "        embeddings = self.embedder(tokens)\n",
    "        encoder_out = self.encoder(embeddings, mask)\n",
    "        logits = self.linear(encoder_out)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "\n",
    "        output = {\"logits\": logits, \"probs\": probs}\n",
    "        if label is not None:\n",
    "            self.accuracy(logits, label)\n",
    "            \n",
    "            output[\"loss\"] = self.loss_function(logits, label)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_metrics(self, reset = False):\n",
    "\n",
    "        return {'accuracy': self.accuracy.get_metric(reset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_id_indexer = SingleIdTokenIndexer(lowercase_tokens=True)\n",
    "reader = StanfordSentimentTreeBankDatasetReader(granularity=\"2-class\", token_indexers={\"tokens\": single_id_indexer}, use_subtrees=True)\n",
    "train_dataset = reader.read('https://s3.amazonaws.com/realworldnlpbook/data/stanfordSentimentTreebank/trees/train.txt')\n",
    "reader = StanfordSentimentTreeBankDatasetReader(granularity=\"2-class\", token_indexers={\"tokens\": single_id_indexer})\n",
    "dev_dataset = reader.read('https://s3.amazonaws.com/realworldnlpbook/data/stanfordSentimentTreebank/trees/dev.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary.from_instances(train_dataset + dev_dataset)\n",
    "print(\"created a vocab,\", vocab)\n",
    "train_dataset.index_with(vocab)\n",
    "dev_dataset.index_with(vocab)\n",
    "train_data_loader = DataLoader(train_dataset,\n",
    "                         batch_sampler=BucketBatchSampler(\n",
    "                             train_dataset,\n",
    "                             batch_size=32,\n",
    "                             sorting_keys=[\"tokens\"]))\n",
    "dev_data_loader = DataLoader(dev_dataset,\n",
    "                         batch_sampler=BucketBatchSampler(\n",
    "                             dev_dataset,\n",
    "                             batch_size=32,\n",
    "                             sorting_keys=[\"tokens\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_TYPE = \"w2v\"\n",
    "if EMBEDDING_TYPE == None:\n",
    "  token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'), embedding_dim=300)\n",
    "                                    \n",
    "  embedding_dim = 300\n",
    "elif EMBEDDING_TYPE == \"w2v\":\n",
    "  embedding_path = \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\"\n",
    "  weight = _read_pretrained_embeddings_file(embedding_path, embedding_dim=300, vocab=vocab, namespace=\"tokens\")\n",
    "  token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'), embedding_dim=300, weight=weight, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
    "EMBEDDING_DIM=300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = PytorchSeq2VecWrapper(torch.nn.LSTM(EMBEDDING_DIM, hidden_size=512, num_layers=2, batch_first=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LstmClassifier(word_embeddings, encoder, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/Text_Attack/tmp_probs/w2v_model.th'\n",
    "vocab_path = '/Text_Attack/tmp_probs/w2v_vocab'\n",
    "vocab = Vocabulary.from_files(vocab_path)\n",
    "model = LstmClassifier(word_embeddings, encoder, vocab)\n",
    "with open(model_path, 'rb') as f:\n",
    "  model.load_state_dict(torch.load(f))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.index_with(vocab)\n",
    "dev_dataset.index_with(vocab)\n",
    "train_data_loader = DataLoader(train_dataset,\n",
    "                         batch_sampler=BucketBatchSampler(\n",
    "                             train_dataset,\n",
    "                             batch_size=32,\n",
    "                             sorting_keys=[\"tokens\"]))\n",
    "dev_data_loader = DataLoader(dev_dataset,\n",
    "                         batch_sampler=BucketBatchSampler(\n",
    "                             dev_dataset,\n",
    "                             batch_size=32,\n",
    "                             sorting_keys=[\"tokens\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dev_data_loader:\n",
    "  print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " model.train().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_grads= []\n",
    "def extract_grad_hook(module, grad_in, grad_out):\n",
    "  extracted_grads.append(grad_out[0])\n",
    "\n",
    "def add_hooks(model):\n",
    "   for module in model.modules():\n",
    "     if isinstance(module, TextFieldEmbedder):\n",
    "       for embed in module._token_embedders.keys():\n",
    "         module._token_embedders[embed].weight.requires_grad = True\n",
    "       module.register_backward_hook(extract_grad_hook)\n",
    "add_hooks(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_weight(model):\n",
    "  for module in model.modules():\n",
    "    if isinstance(module, TextFieldEmbedder):\n",
    "      for embed in module._token_embedders.keys():\n",
    "        embedding_weight = module._token_embedders[embed].weight.cpu().detach()\n",
    "      return embedding_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weight = get_embedding_weight(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "universal_perturb_batch_size = 128\n",
    "dataset_label_filter = '0'\n",
    "targeted_dev_data = []\n",
    "for instance in dev_dataset:\n",
    "  if instance['label'].label == dataset_label_filter:\n",
    "    targeted_dev_data.append(instance)\n",
    "model.get_metrics(reset=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data_loader = DataLoader(targeted_dev_data, batch_size=128, shuffle=False )\n",
    "for batch in dev_data_loader:\n",
    "   batch = move_to_device(batch, cuda_device=0)\n",
    "   model(batch['tokens'], batch['label'])\n",
    "print(\"the accuracy without triggers:\")\n",
    "model.get_metrics()['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_word_saliency(doc, batch, input_y):\n",
    "  batch = move_to_device(batch, cuda_device=0)\n",
    "  original_tokens = batch['tokens']['tokens']['tokens']\n",
    "  word_saliency_list = []\n",
    "  max_len = len(original_tokens[0])\n",
    "  original_vector = batch['tokens']['tokens']['tokens']\n",
    "  #print(original_vector)\n",
    "  origin_prob = (model(batch['tokens'], batch['label']))['probs'][0][1] #the output of true label(label 1)\n",
    "  for position in range(max_len):\n",
    "    if position >= max_len:\n",
    "      break\n",
    "    without_word_vector = deepcopy(original_vector)\n",
    "    #print(without_word_vector)\n",
    "    without_word_vector[0][position] = 1 # changes to <unk> token\n",
    "    batch['tokens']['tokens']['tokens'] = without_word_vector\n",
    "    prob_without_word_vector = (model(batch['tokens'], batch['label']))['probs'][0][1]\n",
    "    word_saliency = origin_prob - prob_without_word_vector\n",
    "    word_saliency_list.append((position, doc[position], word_saliency, doc[position].tag_))\n",
    "  position_word_list = []\n",
    "  for word in word_saliency_list:\n",
    "    position_word_list.append((word[0], word[1]))\n",
    "  return position_word_list, word_saliency_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "dev_data_iter = DataLoader(targeted_dev_data, batch_size=1, shuffle=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_batch_to_text(batch, vocab):\n",
    "  tokens_id = batch['tokens']['tokens']['tokens'].squeeze(0) # torch.Size([37]) or torch.size([14]) or ...  \n",
    "  tokens_id = tokens_id.tolist()\n",
    "  tokens = []\n",
    "  for idx in tokens_id:\n",
    "    tokens.append(vocab.get_token_from_index(idx))\n",
    "  #print(tokens)\n",
    "  # convert tokens to text\n",
    "  text = \" \".join(map(str,tokens))\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_classes(batch, model):\n",
    "  batch = move_to_device(batch, cuda_device=0)\n",
    "  predict_classes = (model(batch['tokens'], batch['label'])['probs']).squeeze()\n",
    "  predict_classes_nump = predict_classes.cpu().detach().numpy()\n",
    "  adv_y = np.argmax(predict_classes_nump)\n",
    "  return adv_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_probs(batch, model):\n",
    "  batch = move_to_device(batch, cuda_device=0)\n",
    "  predict_classes = (model(batch['tokens'], batch['label'])['probs']).squeeze()\n",
    "  predict_classes_nump = predict_classes.cpu().detach().numpy()\n",
    "  return predict_classes_nump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_paraphrase(batch, model, embedding_matrix, true_y=1, verbose=True):\n",
    "  \n",
    "  #text = convert_batch_to_text(batch, vocab)\n",
    "  #print(text)\n",
    "\n",
    "  doc = nlp(text)\n",
    "  input_y = 1\n",
    "  position_word_list, word_saliency_list = evaluate_word_saliency(doc, batch, input_y)\n",
    "  print(word_saliency_list)\n",
    "  averaged_grad = get_averaged_grad(model, batch)\n",
    "  perturbed_text, perturbed_batch, sub_rate, change_tuple_list = Hotflip_PWWS(doc, batch, averaged_grad, embedding_matrix,true_y, word_saliency_list, verbose=verbose, increase_loss=True)\n",
    "  \n",
    "  perturbed_y = predict_classes(perturbed_batch, model)\n",
    "  return perturbed_text, perturbed_y, sub_rate, change_tuple_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_batch(model, batch):\n",
    "    \"\"\"\n",
    "    Takes a batch of classification examples (SNLI or SST), and runs them through the model.\n",
    "    If trigger_token_ids is not None, then it will append the tokens to the input.\n",
    "    This funtion is used to get the model's accuracy and/or the loss with/without the trigger.\n",
    "    \"\"\"\n",
    "    batch = move_to_device(batch, cuda_device=0)\n",
    "    original_tokens =batch['tokens']['tokens']['tokens']\n",
    "    output_dict = model(batch['tokens'], batch['label'])\n",
    "    batch['tokens']['tokens']['tokens'] = original_tokens\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_averaged_grad(model, batch, target_label=None):\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # prepend triggers to the batch\n",
    "    original_labels = batch['label'].clone()\n",
    "    if target_label is not None:\n",
    "        batch['label'] = int(target_label) * torch.ones_like(batch['label']).cuda()\n",
    "    global extracted_grads\n",
    "    extracted_grads = [] # clear existing stored grads\n",
    "    loss = evaluate_batch(model, batch)['loss']\n",
    "    loss.backward()\n",
    "    # index 0 has the hypothesis grads for SNLI. For SST, the list is of size 1.\n",
    "    grads = extracted_grads[0].cpu()\n",
    "    batch['label'] = original_labels # reset labels\n",
    "\n",
    "    # average grad across batch size, result only makes sense for trigger tokens at the front\n",
    "    #averaged_grad = torch.sum(grads, dim=0)\n",
    "    averaged_grad = grads # return just trigger grads\n",
    "    return averaged_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_perturbed_batch(perturbed_batch, position_substitute):\n",
    "  origin = perturbed_batch.copy()\n",
    "  for (position, substitute) in position_substitute:\n",
    "    substitute = torch.from_numpy(substitute) # convert substitute which numpy array to tensor\n",
    "    perturbed_batch['tokens']['tokens']['tokens'][0][position] = substitute\n",
    "    print(perturbed_batch)\n",
    "  return perturbed_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hotflip_PWWS(doc,batch, averaged_grad, embedding_matrix, true_y, word_saliency_list=None, rank_fn=None, halt_condition=None, verbose=True, increase_loss=True):\n",
    "  def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    softmax_x = exp_x / np.sum(exp_x)\n",
    "    return softmax_x\n",
    "  def halt_condition(perturbed_batch, model, true_y=1):\n",
    "    perturbed_batch = move_to_device(perturbed_batch, cuda_device=0)\n",
    "    predict_classes = (model(perturbed_batch['tokens'], perturbed_batch['label'])['probs']).squeeze()\n",
    "    predict_classes_nump = predict_classes.cpu().detach().numpy()\n",
    "    adv_y = np.argmax(predict_classes_nump)\n",
    "    if adv_y != true_y:\n",
    "      return True\n",
    "    else:\n",
    "      return False\n",
    "\n",
    "  perturbed_batch = batch\n",
    "  perturbed_doc = doc\n",
    "  perturbed_text = perturbed_doc.text\n",
    "\n",
    "  substitute_count = 0  # calculate how many substitutions used in a doc\n",
    "  substitute_tuple_list = []  # save the information of substitute word\n",
    "  saliency_array = [word_tuple[2] for word_tuple in word_saliency_list]\n",
    "  word_saliency_array = np.array(saliency_array)\n",
    "  word_saliency_array = [x.cpu().detach() for x in word_saliency_array]\n",
    "  word_saliency_array = softmax(word_saliency_array)\n",
    "  #print(word_saliency_array)\n",
    "\n",
    "  averaged_grad = averaged_grad.cpu()\n",
    "  embedding_matrix = embedding_matrix.cpu()\n",
    "  gradient_dot_embedding_matrix = torch.einsum(\"bij,kj->bik\", averaged_grad, embedding_matrix)\n",
    "  #print(gradient_dot_embedding_matrix)\n",
    "  if not increase_loss:\n",
    "    gradient_dot_embedding_matrix *=-1\n",
    "  best_grad, best_id = torch.topk(gradient_dot_embedding_matrix,1,dim=2)\n",
    "  grads = best_grad.detach().cpu().numpy()[0] # [max_len, 1] -> if k=40 --> [max_len, 40]\n",
    "  substitute = best_id.detach().cpu().numpy()[0] # [max_len, 1]\n",
    "  for (position, token, word_saliency, tag) in word_saliency_list:\n",
    "    substitute_tuple_list.append((position, token.text, substitute[position], grads[position]*word_saliency_array[position], token.tag_)) #substitute is numpy array so substitute[position] is a number\n",
    "  sorted_substitute_tuple_list = sorted(substitute_tuple_list, key=lambda t:t[3], reverse=True)\n",
    "  print('sorted_substitute_tuple_list is:')\n",
    "  print(sorted_substitute_tuple_list)\n",
    "  change_tuple_list = []\n",
    "  position_substitute = []\n",
    "  for (position, token, substitute, score, tag) in sorted_substitute_tuple_list:\n",
    "    print(\"posi is:\")\n",
    "    print(position)\n",
    "    change_tuple_list.append((position, token, substitute, score, tag))\n",
    "    position_substitute.append((position, substitute))\n",
    "    perturbed_text = convert_batch_to_text(perturbed_batch, vocab)\n",
    "    \n",
    "    perturbed_batch = compile_perturbed_batch(perturbed_batch, position_substitute) # substitute whtch coming from sorted_substitute_tuple_list is having the substitute[position] from substitute_tuple_list\n",
    "  \n",
    "    perturbed_text = convert_batch_to_text(perturbed_batch, vocab)\n",
    "    \n",
    "    perturbed_doc = nlp(perturbed_text)\n",
    "    substitute_count += 1\n",
    "    perturbed_batch = perturbed_batch\n",
    "  \n",
    "    if halt_condition(perturbed_batch, model):\n",
    "      if verbose:\n",
    "        print(\"use\", substitute_count)\n",
    "      sub_rate = substitute_count / len(doc)\n",
    "      return perturbed_text, perturbed_batch, sub_rate, change_tuple_list\n",
    "  if verbose:\n",
    "    print(\"use\", substitute_count)\n",
    "  sub_rate = substitute_count / len(doc)\n",
    "  print('sub_rate is')\n",
    "  print(sub_rate)\n",
    "  return perturbed_text, perturbed_batch, sub_rate, change_tuple_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "successful_perturbations = 0\n",
    "failed_perturbations = 0\n",
    "true_y = 1\n",
    "sub_rate_list = []\n",
    "\n",
    "for batch in dev_data_iter:\n",
    "  sub_rate = 0\n",
    "  if batch['label'] == predict_classes(batch, model):\n",
    "    text = convert_batch_to_text(batch, vocab)\n",
    "    print(text)\n",
    "    adv_doc, adv_y, sub_rate, change_tuple_list = adversarial_paraphrase(batch, model, embedding_weight, true_y=1, verbose=True)\n",
    "    print(\"adv_y is\")\n",
    "    print(adv_y)\n",
    "    print(\"true_y is:\")\n",
    "    print(true_y)\n",
    "    if adv_y != true_y:\n",
    "      successful_perturbations +=1\n",
    "      print('successfule example crafted!')\n",
    "    else:\n",
    "      failed_perturbations +=1\n",
    "      print('failed!')\n",
    "    text = adv_doc\n",
    "    sub_rate_list.append(sub_rate)\n",
    "    mean_sub_rate = sum(sub_rate_list)/ len(sub_rate_list)\n",
    "print(failed_perturbations)\n",
    "accuracy = failed_perturbations / len(dev_data_iter)\n",
    "print(\"the accuracy is:\")\n",
    "print(accuracy)\n",
    "print(\"success rate is:\")\n",
    "print(successful_perturbations/len(dev_data_iter))\n",
    "print(sub_rate_list)\n",
    "print(\"mean_sub_rate is:\")\n",
    "mean_sub_rate = sum(sub_rate_list)/ len(sub_rate_list)\n",
    "print(mean_sub_rate)\n",
    "\n",
    "# special thanks to https://github.com/Eric-Wallace/universal-triggers/blob/master/sst/sst.py\n",
    "# https://github.com/JHL-HUST/PWWS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
